### [1] 高性能计算的发展

```
臧大伟, 曹政, 孙凝晖. 高性能计算的发展[J]. 科技导报, 2016, 34(14): 22-28.
```

```
继理论科学和实验科学之后，高性能计算成为人类科学研究的第三大范式。作为科技创新的重要手段，高性能计算广泛应用于核爆模拟、天气预报、工程计算等众多领域，是当代科技竞争的战略制高点，集中体现一个国家的综合实力。本文介绍高性能计算发展的历史和现状，分析当前高性能计算所面临的问题和挑战，探讨高性能计算未来的发展方向。
```

### [2] 话说超级计算

```
李新亮. 话说超级计算[J]. 力学与实践,2022,44(01):243-245.
```

```
介绍了超级计算(大规模并行计算)的基本概念，回顾了近20年来我国超级计算的发展历程。介绍了异构性计算的基本概念和并行计算的基本编程方法。
```

### [3] Current prospects towards energy-efficient top HPC systems

```
Filiposka S, Mishev A, Juiz C. Current prospects towards energy-efficient top HPC systems[J]. Computer Science and Information Systems, 2016, 13(1): 151-171.
```

```
高能效TOP HPC系统的现状与展望

	Abstract. 自从绿色高性能计算机倡议开始以来，顶级超级计算机设计者的地平线上就出现了一个新的设计限制。今天的顶级HPC不仅必须夸耀其艾级性能，还必须考虑以尽可能低的功耗达到新的Exaflop边界。本文的目的是从性能和功耗两个角度介绍顶级超级计算机的现状。使用来自Top和Green HPC排行榜的当前和可用的历史信息，我们确定了最有希望的设计选项，以及当它们组合在一起时如何执行。目前的结果揭示了应成为未来研究重点的主要挑战。
	高性能计算日益成为一种主流的计算模式，不仅适用于科学计算，而且适用于许多其他应用。对于制造商和用户来说，最大限度地发挥其性能(如每秒浮点运算(FLOPS)的数量)是当务之急。每年两次，在国际超级计算大会上，都会编制TOP 500，其中包含了根据高性能Linpack基准(HPL)衡量的最强大的HPC系统。第一份榜单是在1993年发布的，从那时起，世界上最强大的计算系统的峰值性能能力已经增加了5个数量级。但是，增加更高的性能是以高昂的成本为代价的：需要更多的电力来为这些系统供电。目前，TOP 500上最耗电的系统需要近20兆瓦的能源，相当于一家小型火力发电厂的产量。对这些超级计算机施加电力限制的兴趣越来越大，导致了另一个竞争名单的建立，Green 500。该列表使用了相同的基准测试工具，但排名值是性能与所用功率的比率，以每瓦的MFLOPS表示，而不是只关注FLOPS。
	通过回顾列表中的历史变化并分析不同系统特征中的变化，我们可以跟踪导致今天的Petaflop顶级超级计算机的技术发展和设计修改的历史。这两个列表展示了HPC系统开发中的优势和劣势的不同故事。因此，在第一眼看到排名靠前的数据后，我们发现自己想知道，随着更多强大的新计算机进入榜单，顶级超级计算机的排名会随着时间的推移而发生变化。作为这种缓慢退出场景的一个例子，图1中给出了两个列表的动态，通过6年的时间跟踪单个示例系统。从我们观察到的例子中，我们可以得出结论，在最初的几年里，最高性能的超级计算机在Top500榜单上的排名正在慢慢下降，随着年龄的增长，它们的排名下降变得更加明显。与同一系统的绿色性能相比，我们观察到绿色500榜单上的绿色性能几乎稳步下降，这导致了名单上的新来者更经常表现出更高的绿色性能，从而支持了对未来系统的绿色高性能控制设计的认识增加的说法。通过比较过去和现在的这两份清单，可以注意到高性能混凝土发展中的许多重要里程碑。此外，许多技术改进，无论是性能还是功率，都在这些清单上留下了重要的印记。更深入地了解这些清单、它们的趋势和相关性，可以帮助指导研究和行业在未来几年走向亿级超级计算机。对这些系统的详细检查和交叉比较可以挖掘出大量有关节能体系结构的信息，这些体系结构应该在未来得到利用和开发，以进一步改进绿色高性能计算计划。本文的主要目标是通过分析当今领先的计算机系统所展示的技术来推断高性能计算机系统的效率和功耗的未来。目标是洞察当前最佳架构的性能与功率趋势，并确定处理器、互连、系统系列等方面的方向，以显示稳定状态的改进，并为未来节能高效的HPC系统铺平道路。
	自1993年建立以来，500强榜单已经成为高性能计算行业、研究和应用领域的主要参与者衡量其进展的舞台。使用的唯一指标是最大性能(Rmax)，以Flop为单位。这场竞争激烈的开发竞赛导致了对电力需求巨大的系统的构建，这些系统需要维持或提高其性能。认识到设计耗能高的高性能计算机系统的不可持续性，近年来，致力于高能效的高性能计算机设计的努力正在上升。这一绿色的HPC倡议导致了一项新名单的提议，即绿色500。这项提议的主要思想是建立额外的、节能的指标来对最强大的HPC系统进行排名。自2007年以来，与TOP500榜单平行的是《绿色500强》。随着时间的推移，这两份清单都成为高性能计算系统开发的丰富历史数据来源，可以用来更好地解释当今的技术和预测未来的趋势。因此，Green500名单被用作对未来超级计算机的电力消耗进行第一次预测的基础。然而，目前的结论大多是基于动力或绩效与绿色效率之间的相互依赖关系的分析，这意味着预测是基于分别分析绿色指标和用于计算绿色指标的两个变量之一之间已经存在的内在关系的函数。因此，结果主要是由于指标之间存在的自然相关性，正如我们在本文中进一步介绍的那样。类似的方法也可以在中看到，其中作者构建了略有不同的复合指标，这也是基于性能和效率，而在实验中，对Green500列表中使用的功率测量方法进行了验证，使可以使用列表中的信息进行的分析具有现实的权重。在考虑大多数HPC系统的构建块时，尝试分析系统组件的能效时，还可以考虑另一个不同的基准。基于标准SPEC基准的SPECpower_ssj2008将自己确立为标准的服务器能耗基准。它使用特殊的方法来测量基准测试过程中的功耗。基准测试结果是在使用典型的服务器端Java应用程序处理业务事务时，不同负载级别的功耗与所获得的性能分数相关。所有不同负载水平的结果被用来计算总体功率性能指标。中提供了对2010年SPECpower_ssj2008榜单的深入分析。不幸的是，SPECpower结果数据库没有定期更新，这使得该数据集不完整、过时且难以使用。此外，一些最重要的供应商最近似乎已经停止发布他们的SPECpower结果。由于这些原因，顶级列表似乎仍然是能够提供顶级超级计算机体系结构、性能和特征的当前状况的唯一信息来源。高性能计算系统的整体效率取决于它的许多要素。为了能够更深入地反映高性能计算系统的不同组件对整体效率的贡献，已经做出了努力来改进FLOPS/WAT度量。其中一项建议是在2009年引入的绿色指数。这种方法背后的想法是拥有一个单一的数值，该数值将捕获整个系统的能效。
	从2013年开始，Green500增加了其他数据，如：系统系列、互连等。尽管这些数据已经出现在TOP500列表中，但由于信息不完整和不同系统之间的模棱两可，两个列表的匹配很困难，使得整体性能/功率分析繁琐且容易出错。在这些最新增加的基础上，给出了各种设计方案对效率的影响，以及一些初步的历史趋势、最有希望的设计和应该指导未来研究工作的主要挑战。在这篇文章中，我们更新和扩展了开始的工作，以阐明最新的进展以及描述顶级HPC系统的不同参数之间的相互依赖关系。
	Conclusion. 通过对高性能混凝土系统绿色效率的分析，得出了高性能混凝土建筑设计中关键决策的几个重要结论，为实现最低能耗的最佳性能铺平了道路。为了确保未来高性能混凝土主流技术的绿色性能，设计人员应专注于构建将缩小现有理论性能与已实现性能之间差距的异质系统。我们的分布分析证实了这种方法的可行性，表明自绿色倡议开始以来，与电力消耗的增加相比，性能的增长率更高。由于功耗对绿色排名的影响很大，而它本身又主要是由于“纯”核的数量，所以未来绿色高性能计算系统的计算能力应该主要使用加速器核(90%以上)来实现，这些加速器核只会积极地诱导高Rmax。然而，为了确保加速器核心将提供所追求的性能，必须仔细选择系统的其余部件。各种HPC设计选项的交叉比较指向Haswell低功耗高性能处理器系列，在精心设计的系统系列机箱中结合InfiniBand互连，该系统系列机箱采用基于液体的单个元件冷却以及热平衡调度器。然而，仍然有其他开放的问题必须解决，主要问题是简化应用程序开发和高效地移植到异类环境。
```

### [4] A methodology for full-system power modeling in heterogeneous data centers

```
Canuto M, Bosch R, Macias M, et al. A methodology for full-system power modeling in heterogeneous data centers[C]//Proceedings of the 9th International Conference on Utility and Cloud Computing. 2016: 20-29.
```

```
当前数据中心对能源意识的需求鼓励使用电力建模来估计其电力消耗。然而，现有模型存在明显的局限性，这使得它们依赖于应用程序、依赖于平台、不准确或计算复杂。在本文中，我们提出了一种与平台和应用无关的方法，用于在异构数据中心中进行全系统功耗建模，以克服这些限制。它通过系统地选择一组最小的资源使用情况指标并提取它们之间的复杂关系来获取系统中所有资源对能源消耗的影响，从而为具有不同资源使用和能源消耗模式的不同类型的应用程序提供高精度的每个平台的单一模型。我们通过为具有非常不同的功耗配置文件的不同平台生成电源模型来演示我们的方法。我们对真实云应用的验证实验表明，这种模型具有很高的精度(平均估计误差约为5%)。

在本文中，我们提出了一种与平台和应用无关的方法，用于异类数据中心的全系统功耗建模，方法是在运行特定培训工作负载时收集功耗和资源使用情况测量数据，并通过机器学习对其进行拟合。我们的方法克服了前人工作的局限性。它通过系统地选择最小的一组资源使用指标并提取它们之间的复杂关系来得出每个平台的单一模型，这些关系反映了所有资源对能源消耗的影响。我们已经通过为具有非常不同的功耗配置文件的不同平台生成功率模型来演示我们的方法，从基于Intel Xeon和AMD皓龙的高性能服务器架构到基于Intel Atom和ARM Cortex-A的低功耗架构。我们对以CloudSuite和NAS基准为代表的不同资源使用和能耗模式的真实云应用程序进行的实验表明，此类模型提供了高精度(平均估计误差的5%左右)，并且功耗估计与执行时间图中的实际功率测量结果密切相关。作为我们未来工作的一部分，我们将增强我们的方法，以对单个虚拟机和容器的功耗进行建模。我们还计划使用电源模型来驱动节能调度策略，以选择最方便的主机来执行每个应用程序。
```

### [5]Preliminary Investigation of Mobile System Features Potentially Relevant to HPC

```
Pruitt D D, Freudenthal E A. Preliminary investigation of mobile system features potentially relevant to HPC[C]//2016 4th International Workshop on Energy Efficient Supercomputing (E2SC). IEEE, 2016: 54-60.
```

```
能源消耗在科学计算中的重要性日益增加，促使人们对开发高效节能的高性能系统产生了兴趣。移动计算的能量限制推动了能够支持各种计算密集型用户界面和应用的低功耗计算系统的设计和发展。其他人观察到移动设备也在进化，以提供高性能。他们的工作主要检验了计算密集型科学程序的性能和效率，这些程序要么在移动系统上执行，要么在非移动(有时是HPC)系统中嫁接的移动CPU的混合系统上执行。这份报告描述了对五个高性能和移动系统上的单个科学代码的性能和能源消耗的调查，目的是确定各种建筑特征的性能和能效影响。这项初步研究的结果表明，与系统体系结构的其他特定方面相比，ISA在实现高性能和高效率方面没有那么重要。这项研究中采用的策略可以扩展到具有各种存储器访问、计算和通信特性的其他科学应用程序。

ISA与实现特性的相关性：如所示，观察到的性能和能效差异主要是由于浮点子系统的实现特性、内存层次结构、时钟频率和流水线深度，而不是ISA。在COMD的情况下，性能差异似乎主要归因于浮点吞吐量的差异。这表明，与其他实现特征相比，指令集族的总体特征(例如，RISC/CISC，甚至字长)与能量消耗和性能的相关性可能较小。对HPC ARM系统的担忧：HPC ARM的浅缓存层次结构因来自COMD的缓存未命中而变得足够拥挤，从而在高度并行的情况下显著降低了吞吐量。COMD具有很高的缓存命中率。L1缓存命中率较低的应用程序可能会在L2缓存上产生更多拥塞，因此被限制为甚至更低的并行效率。可能具有高性能和在当前移动平台上的应用程序的特征：我们惊讶地发现，尽管移动系统结合了低电压信令，但我们检查的所有系统上的DRAM通信信道都具有类似的性能特征。与HPC系统合并四个信道相比，移动系统合并了其中的一个或两个。由于Comd提供的内存和浮点负载远远低于所有移动平台的吞吐量，因此，与HPC Intel上的吞吐量相比，产生更高内存和浮点吞吐量的应用程序很可能在移动平台上执行时只会增加速度减慢。其产生浮点流量的速率低于COMD的应用程序在移动平台上的执行吞吐量可能与英特尔HPC相似。与HPC相关的移动系统的特征：空闲功率是本研究中考察的HPC和移动系统之间的主要区别。我们观察到，移动系统结合了具有节能功能的网络、I/O和存储器接口，例如低电压信令和各种形式的低功率待机模式。这些节能功能通常不会出现在HPC系统上。这一设计决策在两个方面对能源效率产生了不利影响。如果I/O接口在空闲时占用其有效功率的很大一部分，则会增加整个系统的空闲功率需求。如结果部分所述，竞速到空闲策略可能会产生最高的效率，其副作用是降低整个系统的能效。此外，如果系统在功率上限下运行，将功率分配给未充分利用的设备将减少可用于支持有用计算的能源量。因此，如果未来的HPC系统结合了移动系统中常见的策略，从而降低了整个计算系统中未充分利用的组件的能量消耗，那么未来HPC系统的能源效率可能会显著提高。

ISA:指令集体系结构(Instruction Set Architecture, ISA)，简称体系结构或系统结构(architecture),它是软件和硬件之间接口的一个完整定义。ISA定义了一台计算机可以执行的所有指令的集合，每条指令规定了计算机执行什么操作，所处理的操作数存放的地址空间以及操作数类型。
```

### [6]A survey of high-performance computing scaling challenges

```
Geist A, Reed D A. A survey of high-performance computing scaling challenges[J]. The International Journal of High Performance Computing Applications, 2017, 31(1): 104-113.
```

```
20年前，当商品集群首次出现时，它们给高性能计算带来了革命性的变化。随着规模和复杂性的增加，在可靠性和系统复原力、能效和优化以及软件复杂性方面出现了新的挑战，这表明有必要重新评估目前的办法。本文回顾了最先进的技术，并反思了在构建跨千万亿级计算系统时可能面临的一些挑战，使用从操作经验和社区辩论中获得的见解和观点。

先进计算的世界与中等规模的商品集群还是一个新概念的时候有很大不同。我们在使用商品组件进行大规模系统设计和运行方面学到了很多经验。然而，这些同样的成功也给跨百万亿的政权带来了令人望而生畏的新挑战。随着跨千万亿级系统的节点数量增长到数万个，而且拟议的亿级系统可能包含数十万个节点和十亿路并行，完全可靠的硬件操作的假设变得不那么可信。虽然单个组件(即处理器、磁盘、内存、电源、风扇和网络)的MTBF很高，但大量的总组件数量意味着组件故障现在和将来都是经常关注的问题。复原力的系统设计、部件验证的制造和测试以及算法和软件复原力都是规模化操作复原力的基本要素。不断增加的系统规模带来了围绕能源可用性和成本的互补挑战，预计系统将消耗数十兆瓦的电力。在这种规模下，能源可获得性和成本都成为重要的设计和运营约束，这体现在作业调度策略和资源分配上。此外，研究人员资源分配和使用的成本不仅仅是抽象的。它们是真正的经济选择，对HPC系统运营商和研究资助机构具有影响。在能源效率挑战的推动下，跨百兆时代出现了两种不同的建筑设计。一种是以包含多核CPU和加速器的少量异类节点为代表，另一种是以数量较多的同构节点为代表。为了使未来的亿级系统可用，我们似乎必须开发新的设计方法和运行原则，以体现大型系统的三个重要现实：(A)频繁的硬件部件故障是正常运行的一部分；(B)必须与性能和弹性一样仔细地管理能源消耗和电力成本；以及(C)必须管理软件的复杂性，以降低软件开发成本。在每种情况下，都可以从商业云计算中学到关于组件调整和优化、能源管理和效率以及编程效率的潜在经验教训。许多架构和软件问题在这两个领域都是共同的。
```

### [7] Performance of Devito on HPC-Optimised ARM Processors-big

```
Senger H, de Souza J F, Gomi E S, et al. Performance of Devito on HPC-Optimised ARM Processors[J]. arXiv preprint arXiv:1908.03653, 2019.
```

```
我们在ARM ThunderX2处理器上评估了Devito的性能，这是一种用于有限差分的域特定语言(DSL)。使用两个常见的地震计算内核进行的实验表明，与其他Intel Xeon处理器相比，ARM处理器可以提供与之相当的性能。

结果表明，基于ARM的处理器能够提供与最先进的Intel Xeon处理器类似的性能来执行地震反问题。此外，Devito被证明能够为ARM处理器生成高效的高性能代码。所有模型都成功编译和运行，无需特定于体系结构的代码调优即可实现高性能。
```

### [8] Is Arm software ecosystem ready for HPC?

```
Banchelli Gracia F F, Ruiz D, Hao Xu Lin Y, et al. Is Arm software ecosystem ready for HPC?[C]//SC17: International Conference for High Performance Computing, Networking, Storage and Analysis. 2017.
```

```
近年来，HPC社区对ARM架构的兴趣与日俱增，研究项目主要针对基于ARM的集群的安装。最先进的研究项目的例子有欧洲的万宝龙、日本的后K和英国的GW4/EPSRC。通常主要关注硬件平台，随着硬件通过借用移动市场(例如Big.LITTLE)的解决方案和ARMv8-A可伸缩向量扩展(SVE)技术等附加技术向HPC工作负载发展，ARM HPC社区正在增长。然而，成熟的软件生态系统的可用性和运行大型且复杂的HPC应用程序的可能性在新技术的整合过程中发挥着关键作用，特别是在像HPC这样保守的市场。因此，在这张海报中，我们对ARM系统软件生态系统进行了初步评估，仅限于ARM HPC编译器和ARM性能库，并移植和测试了三个相当复杂的HPC代码套件：QuantumESPRESSO、WRF和FENICS。这些代码的选择并不完全是随机的：事实上，它们已经在ISC的最后两届学生集群竞赛中被提出为HPC挑战赛，所有作者都参与了基于ARM的集群的运营，并获得了球迷最喜欢的奖项。

关于编译器，在串行环境中，我们注意到ARM HPC编译器在涉及矩阵-矩阵运算的基准测试中的性能显著提高，平均而言，我们重现了已经研究过的行为。在并行环境中，我们注意到在使用ARM HPC编译器时，在更多核心数和细粒度并行的情况下性能更好。这似乎与在ARM HPC编译器的OpenMP运行时创建/销毁线程的开销较低有关。在存在更粗粒度的并行性的情况下，线程创建/销毁的开销不那么显著，两个编译器有类似的趋势，略有优势∼20%，有利于GCC。在数学库方面，在DGEMM微基准测试中使用ARM性能库2.2.0获得的性能与其他优化库在小矩阵尺寸下提供的性能相当。当矩阵大小大于1500×1500个元素时，可以显著提高性能。ARM性能库还可能过早实现与FFT相关的函数，这些函数可以在调用FFTW库函数的时刻得到补偿。大型生产HPC代码的评估是完美的，使我们能够构建和执行两个模块化套件WRF和FENICS，支持所有可用的并行性，没有重大障碍。总的来说，ARM软件生态系统已经被证明足够成熟，即使在学生集群竞赛这样的教育项目中，也可以在基于ARM的集群上运行而不受主要限制。在性能方面，结果与其他主流可比软件(如GCC，ATLAS，BLAS，OpenBLAS)一致，但有两个例外：i)ARM工具允许在矩阵-矩阵运算中达到显著更好的性能，而ii)FFT部分应进一步优化工作。
```

### [9] Comparing Allinea"s and Intel"s Performance Tools for HPC

```
Luecke G R, Groth B M, Weeks N T, et al. Comparing Allinea's and Intel's performance tools for HPC[C]//Proceedings of the 25th High Performance Computing Symposium. 2017: 1-12.
```

```
要高效地使用高性能计算机，优化应用程序以获得高性能至关重要。要实现这一点，HPC开发人员必须利用性能工具在大型、复杂的科学应用程序中发现并纠正性能问题。Allinea和英特尔提供供应商支持的性能工具，这些工具会定期更新，以获取最新硬件上的重要性能指标。在本文中，作者对Allinea的MAP性能工具和Intel的性能工具进行了评估和比较，以帮助进行应用程序优化。作者发现，Allinea的地图提供了使用直观、易于使用的用户界面诊断和修复性能问题所需的有用性能指标。英特尔的性能工具以更复杂的用户界面为代价，提供了更详细和可定制的应用程序性能视图。本文提供的比较将帮助HPC开发人员确定哪种性能工具最适合他们。

使用混合MPI+OpenMP版本的EPSNP，作者评估了Allinea和Intel Performance Tools的功能和易用性。就我们的功能标准而言，得分最高的工具是MAP，因为它包含应用程序优化所需的功能。MAP有一个用于检测MPI负载不平衡的Yes∗，因为Metric View可以暗示负载不平衡，但不像ITAC中那样提供直接检测。英特尔跟踪分析器和采集器(ITAC)在这一类别中得分较低，因为它缺乏详细的源代码支持、内存指标和I/O指标。然而，作者认为ITAC的MPI功能，特别是事件时间线图表和负载平衡图表特别有用。

要使用英特尔的性能工具评估MPI应用程序的性能，必须使用两个工具：用于MPI通信的ITAC和用于确定源代码中的瓶颈的VTune或PFLET。因此，Allinea map是一个工具，它同时提供MPI通信的性能数据和源代码级的评测信息。作者认为这是MAP相对于ITAC或VTune的优势。此外，作者认为MAP的用户界面比ITAC或VTune的不同界面更直观。Allinea还有一个高级的、易于使用的性能工具，称为性能报告。此工具不需要重新编译。综合起来，ITAC和VTune摘要页提供的信息与性能报告类似，但这需要应用程序运行两次(每个工具运行一次)。此外，性能报告会提供性能改进建议，而英特尔的工具则不会。作者认为这些建议是有价值的，并希望其他工具也能实现类似的分析，以帮助程序优化。但是，要在使用性能报告后发现性能问题，可能需要使用其他需要编译源代码的工具进行额外的运行。图形用户界面的响应性是另一个重要的考虑因素。与X11转发相比，MAP的远程客户端显著提高了可用性，因为它减少了通信延迟。它还减少了集群登录节点上的负载，因为图像渲染将移动到用户的工作站。在表3这张表中，我们给出了两个等级：满意(S)和需要改进(NI)。在这里，性能报告和英特尔性能函数或循环执行时间(PFLET)不使用图形用户界面，因此它们被归类为N/A。
```

### [10] 华为鲲鹏920: 一颗勇敢的“芯”

```
苏月. 华为鲲鹏 920: 一颗勇敢的 “芯”[J]. 计算机与网络, 2019, 21.
```

```
```

### [11] Benchmarking the first generation of production quality Arm-based supercomputers

```
McIntosh‐Smith S, Price J, Poenaru A, et al. Benchmarking the first generation of production quality Arm‐based supercomputers[J]. Concurrency and Computation: Practice and Experience, 2020, 32(20): e5569.
```

```
在本文中，我们展示了两台生产质量超级计算机的扩展结果，它们使用第一代基于 Arm 的 CPU，这些 CPU 已针对科学工作负载进行了优化。两种系统均使用 Marvell ThunderX2 CPU，可提供高核心数和一流的内存带宽。第一个系统是 Isambard，它是由 GW4 联盟和英国气象局运营的 Cray XC50“侦察兵”系统，作为二级国家高性能计算服务。第二个系统是三个基于 Arm 的 HPE Apollo 70 系统之一，作为 Catalyst UK 项目的一部分，在布里斯托大学运行。我们将这两个系统的扩展结果与基于 Intel Skylake 和 Broadwell CPU 的三个 Cray XC50 系统进行比较。我们专注于对英国国家 HPC 服务 ARCHER 和我们的项目合作伙伴很重要的一系列应用程序和迷你应用程序。我们还比较了基于 Arm 的 HPC 系统上可用的最先进工具链的性能和成熟度。

本文提供的结果表明，使用基于 Arm 的处理器构建的超级计算机现在提供的生产性能与现有处理器供应商的最先进产品具有竞争力。我们发现，即使在具有更高峰值浮点性能的基于 x86 的 CPU 可以在低节点数下击败 ThunderX2 的情况下，在适合实际科学运行的实际规模下，ThunderX2 通常也会变得更具竞争力，因为其更大的内存带宽有利于通信表现。我们还看到，大多数代码在 x86 和 ThunderX2 之间以及在具有不同互连技术的两个 ThunderX2 系统之间的缩放比例相似。随着未来基于 Arm 的处理器将通过引入 Arm 可扩展矢量扩展等新技术来提高浮点性能和缓存带宽，这些供应商的 CPU 产品将变得更加引人注目。我们的大多数基准测试都开箱即用地编译和运行，并且不需要针对特定架构的代码调整来实现高性能。这代表了 Arm 高性能计算生态系统成熟的一个重要里程碑，这些处理器现在可以被视为未来生产采购的可行竞争者。总体而言，这些结果表明，针对 HPC 进行了优化的基于 Arm 的服务器 CPU 现在是生产系统的真正选择，可提供与同类最佳 CPU 相媲美的大规模性能，同时可能提供具有吸引力的性价比优势。随着多家系统供应商现在提供使用这些 CPU 作为其标准产品组合的一部分的解决方案，基于 Arm 的处理器有望成为科学计算领域未来的一部分。
```

### [12]ThunderX2 Performance and Energy-Efficiency for  HPC Workloads

```
Calore E, Gabbana A, Schifano S F, et al. ThunderX2 performance and energy-efficiency for HPC workloads[J]. Computation, 2020, 8(1): 20.
```

```
在过去几年中，由于环境、技术和经济原因，HPC 系统的能源效率变得越来越重要。几个项目已经调查了不同处理器和加速器的使用，以寻求能够为数据中心和 HPC 安装实现高能效水平的构建系统。在此背景下，Arm CPU 架构因其在低功耗和低能耗应用中的广泛应用而备受关注，而服务器级处理器则是最近才出现在市场上。在这项研究中，我们以 Marvell ThunderX2 为目标，这是一款最新的基于 Arm 的处理器，旨在满足高性能计算应用的要求。我们的兴趣主要集中在大型 HPC 安装环境中的评估，因此我们使用 ERT 基准和两个 HPC 生产就绪应用程序评估了计算性能和能源效率。最后，我们将结果与大型并行系统中常用的其他处理器进行了比较，并突出了可受益于 ThunderX2 架构的应用程序在计算性能和能源效率方面的特点。为实现这一目标，我们还描述了如何针对 ThunderX2 修改和优化 ERT，以及如何在此处理器上运行应用程序时监控功耗。

在这项研究中，我们分析了 Marwell 针对服务器市场发布的基于 Arm 的 ThunderX2 处理器的性能。我们考虑了计算和能源性能，并评估了哪些类别的 HPC 应用程序更适合充分利用该处理器。为此，我们最初修改了 ERT 基准以针对 Arm 架构进行优化。这是必需的，因为原始代码针对其他指令集（例如 x86）进行了优化，但不适用于 Arm，并且无法充分利用与实现大部分浮点相关的 TX2 融合乘加指令表现。使用 Arm 优化版本的 ERT 基准测试，我们绘制了一个经验 Roofline 图，以比较内存带宽和计算吞吐量与 HPC 系统中常用的两个英特尔处理器。 Roofline plot 显示，TX2 的机器平衡值最低，内存带宽最高，计算性能低于 Skylake，但与 Haswell 相似。这些结果表明 TX2 对于内存密集型应用程序更有效，而对于密集浮点计算则更少。此外，我们还强调了处理器之间的其他架构差异，即英特尔处理器基于更少的内核和更大的 SIMD 矢量指令，而 TX2 使用更小的 SIMD 寄存器和更多的内核。我们的参考应用程序的性能结果是进一步的确认；事实上，内存密集型内核（例如 LBM 传播和 LQCD Dirac）在 TX2 上实现了更好的性能，而计算密集型 LBM 碰撞在 Intel Skylake 上的性能更好。关于能源效率，我们在运行参考应用程序时分析了处理器的功耗。为了收集功率和能量测量值，我们在英特尔处理器上使用了 RAPL 计数器，而在 TX2 上，我们使用了可用的片上功率传感器。对于 TX2，我们详细描述了如何在应用程序代码中使用功率传感器来测量能耗，并将它们的值与我们测试系统主板上可用的片外传感器读数进行比较。这表明这两个值是一致的。从能源的角度对我们的参考应用进行基准测试，我们表明，对于 LBM 等以浮点为主的计算，TX2 的能源消耗水平与 Skylake 相同，几乎是 Haswell 的一半；另一方面，对于 Dirac 等内存受限应用，TX2 的能效要高得多，所需焦耳是 Skylake 所需焦耳的一半，略多于 Haswell 所需焦耳的三分之一。这证实了 TX2 对于在内存和浮点运算之间表现出良好平衡的应用程序显示出更好的计算和能源效率。如前所述，这是一个有趣的特性，因为一些 HPC 应用程序通常受内存限制，并且许多新兴的科学应用程序（例如，在大数据和人工智能领域）表现出较低的 flop-per-byte 比率值，并且在某些情况下— 接近 1。在旁注中，我们注意到，在我们的分析中，我们使用了常见的 HPC 软件工具——编译器、库等——现在它们在支持 TX2 方面已经达到了很好的成熟度，在通用 Arm 架构，允许执行为其他标准商品处理器开发的几乎开箱即用的代码。

总之，我们的工作表明，TX2是未来高性能计算系统的有效替代解决方案，在计算能力和能源效率方面具有竞争力，在总拥有成本方面也具有潜在优势。对于未来的工作，我们计划还考虑到加速器，如GPU。特别是，我们将对托管TX2处理器和GPU的整个HPC节点的评估感兴趣，这些节点应该很快就会推出。这将允许还考虑到评价和比较不同处理器和加速器之间的节点内通信链路。此外，我们还计划在集群级别上研究TX2的性能和能效，包括节点间通信性能的评估，以及我们的应用程序在多节点系统上的可扩展性。
```

### [13] Porting the LHCb Stack from x86 (Intel) to  aarch64 (ARM) and ppc64le (PowerPC)

```
Promberger L, Clemencic M, Couturier B, et al. Porting the LHCb Stack from x86 (Intel) to aarch64 (ARM) and ppc64le (PowerPC)[J]. The European Physical Journal Conferences, 2019, 214:05016.
```

```
大型强子对撞机正在为即将于2021年开始的第三次大型强子对撞机进行数据选择和处理链的重大变化。有鉴于此，已经启动了几项举措来优化软件堆栈。本文讨论了将LHCb堆栈从x86_64体系结构移植到aarch64和ppc64le体系结构，目的是评估用于高级触发器(HLT)的计算基础设施的性能和成本。这需要移植一个包含500多万行代码的堆栈，并找到LCG提供的外部库的工作版本。在所有软件包中，最大的挑战是越来越多地使用矢量化-因为许多矢量化库专门针对x86体系结构，不支持任何其他体系结构。尽管存在这些挑战，我们还是成功地将LHCb高级触发器代码移植到了aarch64和ppc64le上。这篇文章讨论了软件移植的现状和计划，以及以独立于平台的方式处理代码向量化的LHCb方法。

这项研究是使LHCb协作不仅能够在Intel x86_64计算机上运行他们的Run 3 HLT计算群的第一步，也是在ARM aarch64和PowerPC ppc64le上运行的第一步。在整个工作中，第一个关于跨平台支持向量化将是一个主要问题的分析被证明是正确的。然而，由于这项研究可以证明端口是可行的，并且Intel E5-2630 v4和ARM Cavium ThunderX2之间的性价比差异没有因因素而不同，我们相信继续这项工作将有利于HLT计算群投标的多样性和竞争力。
```

### [14] Supercomputing with commodity CPUs: Are mobile  SoCs ready for HPC

```
Rajovic N, Carpenter P M, Gelado I, et al. Supercomputing with commodity CPUs: Are mobile SoCs ready for HPC?[C]//Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 2013: 1-12.
```

```
在20世纪90年代末，强大的经济力量促使商用台式机处理器在高性能计算中得到采用。这种转变是如此有效，以至于2013年6月的TOP500榜单仍然由x86主导。2013年，计算领域最大的大宗商品市场不是个人电脑或服务器，而是移动计算，包括智能手机和平板电脑，其中大部分都是基于ARM的SoC。这导致有人建议，一旦移动SoC提供足够的性能，移动SoC就可以帮助降低HPC的成本。本文对这一问题进行了详细的论述。我们分析了移动SoC性能的趋势，并将其与20世纪90年代的类似趋势进行了比较。我们还介绍了评估移动SoC的性能和效率、部署集群以及评估生产应用程序的网络和可扩展性的经验。总而言之，我们首先回答了移动SoC是否已经为HPC做好了准备。
我们已经证明，移动处理器具有很有前途的特性，使其成为不久的将来高性能计算的候选者。扩展到更多并行节点的应用程序可能会受益于具有竞争力的性能和能效的低成本平台。这些应用要求类似于Bluegene系统的要求。由于移动处理器是由商品组件业务动态驱动的，因此它们拥有更积极的路线图和更快的创新，并受到快速增长的市场的推动，市场对性能的要求越来越高。虽然性能和能耗表明移动SoC正在为HPC做好准备，但在生产系统可行之前，必须解决第6.3节中描述的限制：缺乏ECC保护、互连速度慢、32位地址空间和低等级热封装。所有这些限制都是设计决策，因为HPC和服务器需要缺失的功能，但智能手机和平板电脑不需要。我们鼓励移动SoC供应商考虑在其产品中增加必要的支持。另一个优势是，嵌入式处理器，特别是移动处理器，很可能成为3D存储器封装的早期采用者。这种方法正在考虑用于下一代HPC系统，包括未来的NVIDIA Volta GPU。在移动领域，较高的CPU和内存3D封装成本可以在一个大市场中摊销，而在HPC领域，由于体积较小，更大的成本将转嫁到最终用户身上。一类新的服务器SoC可能会成功，它建立在与移动SoC相同的基本技术之上，并实现缺失的功能。或者，移动供应商可能决定包括面向服务器和HPC市场所需的最小功能集。无论哪种方式，超级计算的成本可能会因为今天的移动SoC的后代而即将下降。
```

### [15] Techniques and tools for measuring energy  efficiency of scientific software applications

```
Abdurachmanov D, Elmer P, Eulisse G, et al. Techniques and tools for measuring energy efficiency of scientific software applications[C]//Journal of Physics: Conference Series. IOP Publishing, 2015, 608(1): 012032.
```

```
近年来，科学高性能计算(HPC)和高吞吐量计算(HTC)的规模显著增加，并对总能耗和成本变得敏感。因此，能源效率已成为高能物理等科学领域的一个重要问题。人们对利用替代架构(如低功耗ARM处理器)来取代传统的Intel x86架构的兴趣日益浓厚。然而，尽管这种解决方案已经成功地用于I/O和内存需求较低的移动应用程序，但尚不清楚它们是否适用于科学计算环境，是否更节能。此外，缺乏工具和经验来推导和比较各种工作负载的架构之间的功耗，并最终支持软件优化以提高能效。为此，我们对在ARM和英特尔架构上运行的HEP应用程序的工作负载进行了多项物理和基于软件的测量，并比较了它们的功耗和性能。我们利用几种分析工具(硬件和软件)来提取不同的用电特征。我们报告了这些测量的结果，以及在开发一套测量技术和分析工具以准确评估科学工作负载的功耗方面获得的经验。

考虑到最近的实验需要大量的计算资源--因此也就是能源--能源效率已经成为HTC的一个主要担忧。考虑到大型强子对撞机目前的要求和成本限制，大型强子对撞机计算是需要高能效设施的一个典型例子。对能源效率的需求促使人们对准确评估HTC系统的不同组件感兴趣，以了解能源是如何以及在哪里消耗的，并提高整体效率。然而，HTC系统是复杂的，由不同的组件组成。在本文中，我们介绍了一些技术和工具，这些技术和工具可以从不同的角度和粒度洞察能量的消耗方式和位置。此外，开源配置工具IgProf已经扩展到运行在64位ARM上，并提供函数级的能量配置功能。使用这些工具和技术，我们还报告了对x86-64和ARMv7处理器的能源性能进行比较的研究，证实了ARMv7对于高效HTC系统的潜力，如果服务器级系统围绕此类芯片构建的话。
```

### [16] Evaluation of mobile ARM-based SoCs for  high performance computing

```
Selinger A, Rupp K, Selberherr S. Evaluation of mobile arm-based socs for high performance computing[C]//Proceedings of the 24th High Performance Computing Symposium. 2016: 1-7.
```

```
当前超级计算机的功耗已成为成功达到亿级的关键限制因素。为了进一步提高能源效率，过去已经提出了使用移动系统芯片(SoCs)。在这项工作中，我们调查了四个配备了不同SoC的开发板，看看它们是否适合高性能计算。每一块板都经过仔细的基准测试，以获得高浮点计算性能、高内存带宽、低延迟和功耗，所有这些都是成功的HPC系统的关键特性。与以前的工作不同，我们还将集成在SoC中的支持OpenCL的图形处理单元包括在我们的基准测试中。我们的基准测试结果表明，移动SoC还没有准备好在HPC中成功采用。虽然理论峰值计算性能和内存带宽表明SoC是现有HPC系统的竞争对手，但实际价值在大多数情况下要低得多。因此，在降低采购成本或提高能效方面，所有SoC都不能为HPC提供有吸引力的选择。

在这项工作中，我们评估了移动SoC对高性能计算环境的适用性。在考虑了原始浮点计算能力、内存带宽和延迟的实际性能后，我们得出结论，移动SoC仅在延迟方面具有竞争力。即使忽略构建基于移动SoC的假想集群所需的额外网络基础设施的成本，SoC上的CPU仍落后于HPC中的大多数现有系统。

目前在移动SoC中向新的ARMv8指令集的迁移预计将通过向64位地址空间的迁移来解决内存容量的限制。此外，通过霓虹灯向量指令预计会有更好的双精度能力。然而，缺乏对纠错码(ECC)主存储器的支持仍将是HPC采用的一个重大障碍。尽管如此，对于无法获得x86许可证的芯片制造商来说，基于ARM的内核仍将是一个有吸引力的选择。
```

### [17] Performance and energy efficiency analysis of HPC  physics simulation applications in a cluster of ARM processors

```
Bez J L, Bernart E E, dos Santos F F, et al. Performance and energy efficiency analysis of HPC physics simulation applications in a cluster of ARM processors[J]. Concurrency and Computation: Practice and Experience, 2017, 29(22): e4014.
```

```
我们分析了使用低功耗的高级RISC机器处理器的非传统集群来执行两个科学并行应用的可行性和能效。为此，我们选择了两个计算和通信成本较高的应用程序：模拟地球物理事件的Ondes3D和模拟天体物理事件的All-Pair N-Body。我们比较和讨论了不同的编译指令和处理器频率的影响，以及它们如何干扰时间到解决方案和能量到解决方案。我们的结果表明，对于高级RISC机器体系结构，通过在编译时正确调整应用程序，我们可以显著减少执行时间和计算模拟所花费的能量。此外，我们观察到，使用双核解决方案的时间缩短了54.14%，解决方案的能源效率提高了53.65%。此外，我们还考虑了两个处理器频率调节器对这些指标的影响。结果表明，节电调速器具有较小的瞬时功耗。然而，它花费更多的时间执行任务，增加了实现解决方案所需的能量。最后，我们用Pareto将实验结果中的能量消耗与执行时间关联起来。这些发现表明，有可能通过调整应用程序和硬件配置来实现能效，从而探索用于高性能计算应用的低性能集群。

在本文中，我们扩展了之前的工作，将能量成本与两个高性能计算应用程序的执行时间关联起来。我们选择了Ondes3D地球物理模拟器和N体问题作为研究的基础。实验是在我们的基于ARM Allwinner A20处理器的8节点实验集群上进行的，并在卢森堡的Grid'5000站点上进行了实验。我们通过调整优化标志和处理器频率来分析不同数量的节点对每个应用程序的影响，以获取我们的测试台和应用程序所能提供的最佳性能。我们已经证明，使用低性能集群来执行Ondes3D和N-Body等科学应用程序是可行的，并在能耗和执行时间之间实现平衡。我们的结果表明，通过正确配置优化指令，可以显著减少应用程序运行时以及它所花费的能量。对于Ondes3D，我们观察到当所有内核都处于活动状态时，执行时间(解决问题的时间)减少了48.19%。在此方案中，还观察到能源到解决方案的收益为46.93%。对于N-Body应用程序，当启用优化标志时，解决方案的时间减少61.09%，并且应用程序在整个集群中执行。此外，与基线相比，能量对溶液的比率也降低了51.94%。考虑到调控器定义的处理器频率，结果显示，使用四核和节电频率管理器时，执行时间最多增加21.37%。虽然处理器的瞬时能耗较小，但由于其工作频率较低，因此执行任务的时间较长。因此，可以理解，只有在处理器空闲或工作负载不大的情况下，频率降低才有用。总结我们的分析，我们表明最初为传统？86平台开发的科学应用程序可以移植到低功耗体系结构上，并呈现低功耗。为此，我们在传统的X86处理器集群上执行Ondes3D应用程序。结果表明，这种方法比在ARM上执行16个进程的速度快7.87倍。然而，该应用程序在ARM集群中达到其解决方案所需的能源远远低于其竞争对手，16个进程的能耗是竞争对手的3.11倍。作为未来的工作，我们希望分析具有16个以上节点的Yggdrasil集群的行为。与ARMv7系列中的其他ARM型号以及较新的ARMv8系列中的ARM A15处理器的比较也被建议作为未来的工作。这一分析可以详细说明嵌入式ARM处理器最近的改进。这项未来的研究还将允许我们检查具有双精度浮点运算支持的霓虹灯单元将如何影响每个节点的性能和能源消耗。将CubieTruck与其他嵌入式平台(如NVIDIA Jetson X1和AMD Opteron A1100)进行比较，并考虑其他功能(例如，CPU和内存特性、网络和磁盘I/O开销)也是很有趣的。
```

### [18] Evaluating ARM HPC clusters for scientific workloads

```
Maqbool J, Oh S, Fox G C. Evaluating ARM HPC clusters for scientific workloads[J]. Concurrency and Computation: Practice and Experience, 2015, 27(17): 5390-5410.
```

```
使用耗电量大的商品服务器构建的现代高性能计算 (HPC) 系统的功耗是实现 Exascale 计算的主要障碍之一。 HPC 社区已经做出了一些努力来鼓励在大规模 HPC 系统中使用低功耗片上系统 (SoC) 嵌入式处理器。这些举措已成功证明了 ARM SoC 在 HPC 系统中的使用，但在为 Exascale 计算提供案例之前，仍然需要分析这些系统在 HPC 平台上的可行性。当前 ARM-HPC 评估的主要缺点包括缺乏对分布式多核系统的性能水平和在 HPC 上运行的大规模应用程序的基准测试的性能水平的详细了解。在本文中，我们对基于 ARM 的 SoC 的服务器和 HPC 基准测试的主要方面的结果进行了全面评估。对于实验，我们构建了一个非常规的 ARM Cortex-A9 集群，称为 Weiser，并运行单节点基准测试（STREAM、Sysbench 和 PARSEC）和多节点科学基准测试（高性能 Linpack (HPL)、NASA Advanced Supercomputing (NAS) Parallel Benchmark 和 Gadget-2)，以便为系统的性能限制提供基准。根据实验结果，我们声称 ARM SoC 的性能在很大程度上取决于内存带宽、网络延迟、应用程序类、工作负载类型以及对编译器优化的支持。在基于服务器的基准测试中，我们观察到，在为数据库事务执行内存密集型基准测试时，x86 对多线程查询处理的性能提高了 12%。但是，ARM 的单核性能功率比提高了四倍，四核提高了 2.6 倍。我们注意到，Java 中模拟的双精度浮点导致性能比 C 中 CPU-bound 基准的性能慢三到四倍。尽管 Intel x86 在面向计算的应用程序中表现稍好，但 ARM 在共享内存基准测试的 I/O 绑定应用程序中表现出更好的可扩展性。我们在 MPJ-Express 运行时中加入了对 ARM 的支持，并对两个广泛使用的消息传递库进行了比较分析。我们在消息传递评估（NBP 和带有 MPJ-Express 和 MPICH 的 Gadget 2）中的集群的网络带宽、大规模应用程序扩展、浮点性能和能效方面获得了类似的结果。我们的研究结果可用于评估基于 ARM 的集群在服务器工作负载和科学工作负载中的能效，并为构建节能 HPC 集群提供指导。

在本文中，我们对基于 ARM 的 SoC 进行了全面评估，并涵盖了服务器和 HPC 基准测试的主要方面。为了提供分析，我们进行了广泛的测量并涵盖了多样化的应用程序和基准测试，包括单节点基准测试（例如，内存带宽（STREAM）、共享内存基准测试（PARSEC）和数据库事务（Sysbench）） ，以及多节点集群基准（例如，HPL、Gadget-2 和 NAS）。根据测量结果，我们发现单节点 ARM SoC 的性能取决于内存带宽、处理器时钟、应用程序类型，而多节点性能在很大程度上取决于网络延迟、工作负载类别以及编译器优化和库使用的优化。在基于服务器的基准测试中，我们观察到在 OLTP 事务等内存密集型基准测试中，英特尔 x86 的多线程查询处理性能水平比 ARM 高 12%。但是，ARM 在单核性能功耗比测试中的表现要好 4 倍，在多核测试中要好 2.6 倍。我们还发现，在 JVM/JRE 中模拟双精度浮点导致 CPU 密集型应用程序中基于 Java 的基准测试的性能要慢三到四倍（与 C 相比）。在共享内存评估期间，英特尔 x86 在 EP 应用程序（例如 Black-Scholes）中显示出明显优于 ARM 的性能优势。然而，对于 I/O 绑定应用程序（例如，Fluidanimate），ARM 显示出更好的 Amdahl 定律缩放效率。我们对使用 NPB 和 Gadget-2 的两个广泛使用的消息传递库（例如 MPICH 和 MPJ-Express）的评估揭示了网络带宽、工作负载类型和消息传递开销对可扩展性、浮点性能、大型规模应用程序和集群的能源效率。我们发现，尽管与商用 HPC 集群相比网络带宽较慢，但我们基于 ARM 的集群实现了 321 MFLOPS/W，略高于 2013 年 11 月 Green500 列表中的第 222 位超级计算机。最后，我们发现当一个ARM 处理器上的 NEON SIMD 浮点单元与手动调整的编译器优化相结合，HPL 的浮点性能比直接（未优化）执行要好 2.5 倍。 ARM 处理器在移动和嵌入式系统市场占有一席之地，通常用于手持设备。然而，这种情况很可能在不久的将来发生改变。从 ARM Cortex-A9 SoC 的多核评估中，我们得出结论，ARM 处理器具有用作轻量级服务器的潜力，并且它们在 I/O 绑定共享内存基准测试中显示出合理的性能水平。基于对 ARM SoC 的分布式内存集群评估，我们能够确认大规模科学模拟的可扩展性以及实现最佳性能水平的不同优化技术。基于 ARM 的 SoC 是数据中心和 HPC 行业对能源效率日益增长的需求的合理解决方案。但是，在 ARM 成为主流之前，仍然存在与软件和硬件支持不佳相关的挑战需要解决。
```

### [19] Characterization and bottleneck  analysis of a 64-bit ARMv8 platform

```
Laurenzano M A, Tiwari A, Cauble-Chantrenne A, et al. Characterization and bottleneck analysis of a 64-bit ARMv8 platform[C]//2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 2016: 36-45.
```

```
本文首次全面研究了适用于HPC工作负载的第一个商用64位ARMv8平台-AppliedMicro X-gene的性能、功耗和能耗。我们的研究包括对X基因与HPC系统中常见的其他三个建筑设计点的详细比较。在这些平台上，我们对400多个工作负载进行了仔细的测量，涵盖了不同的应用领域、并行化模型、浮点精度模型和内存强度。我们发现，X-gene的能耗平均是Intel Sandy Bridge的1.2倍，而Sandy Bridge的平均速度是X-gene的2.3倍。精确量化两个平台之间的性能和能源差异的原因是一个重要但具有挑战性的问题，通常通过详细的模拟来解决，这种方法在向上扩展到完整应用程序和广泛的工作负载组合方面的能力有限。相反，本文采用一种称为偏最小二乘(偏最小二乘)路径建模的统计框架来解决这一问题。PLS路径建模使我们能够捕捉复杂的因果关系和难以测量的性能概念，这些概念与体系结构单元和子系统在使用现成的硬件计数器测量改进应用程序性能方面的有效性有关。我们使用偏最小二乘路径模型来量化X-gene和Sandy Bridge在HPC域中性能差异的原因，发现存储子系统的性能是导致这些差异的主要原因。
本文首先对64位ARMv8平台X-gene进行了详细的描述和体系结构瓶颈分析。通过对四种架构设计和数百个应用测试案例的详细分析，我们发现X-gene的性能与Intel Atom大致相当，能耗与Intel Sandy Bridge大致相当。我们还开发了一种称为偏最小二乘路径建模的复杂统计建模技术的新应用，以揭示X-gene上的体系结构瓶颈，揭示了内存子系统是限制高性能计算应用程序的性能和能源消耗的关键体系结构瓶颈之一。
```

### [20] HPC Benchmarking: Scaling Right and  Looking Beyond the Average

```
Radulovic M, Asifuzzaman K, Carpenter P, et al. HPC benchmarking: scaling right and looking beyond the average[C]//European Conference on Parallel Processing. Springer, Cham, 2018: 135-146.
```

```
设计一个平衡的HPC系统需要了解主要的性能瓶颈。到目前为止，还没有成熟的方法来统一评估HPC系统和工作负载，以量化主要的性能瓶颈。在本文中，我们在一个生产HPC平台上运行了7个生产HPC应用程序，并分析了关键的性能瓶颈：Flop性能和内存带宽拥塞，以及对向外扩展的影响。我们表明，结果在很大程度上取决于执行进程的数量和测量的粒度。因此，我们主张在应用程序套件中指导选择实验的代表性规模。此外，我们还建议用低、中、重度利用率的时间比例来表示FLOPS的性能和内存带宽。我们表明，这提供了比平均水平更准确和更具可操作性的证据。

清楚地了解HPC系统的性能因素和瓶颈，对于设计具有最佳功能和合理成本的HPC基础设施至关重要。这种看法只能通过仔细分析现有的高性能计算系统及其工作负载的执行情况来实现。在执行生产HPC应用程序时，我们的研究结果表明，HPC应用程序的性能指标强烈依赖于执行进程的数量。我们认为，HPC应用程序套件必须指定进程数量的窄范围，才能使结果代表真实世界的应用程序使用。此外，我们还发现，对性能指标和瓶颈的平均测量可能具有很强的误导性。相反，我们建议将性能度量定义为应用程序使用最大持续值的某些部分的执行时间的百分比。总体而言，我们认为这项研究为准确衡量关键性能因素及其对整体HPC性能的影响提供了新的指导方针。
```

### [21] Advanced Performance Analysis of HPC Workloads on  Cavium ThunderX

```
Calore E, Mantovani F, Ruiz D. Advanced performance analysis of HPC workloads on Cavium ThunderX[C]//2018 International Conference on High Performance Computing & Simulation (HPCS). IEEE, 2018: 375-382.
```

```
在过去5年中，人们对基于ARM的平台作为高性能计算解决方案的兴趣显著增加。在这篇文章中，我们展示了，与早期的先锋测试相比，几种应用性能分析技术现在也可以应用于基于ARM的SoC。为了展示现有工具提供的可能性，我们提供了一个对Lattice Boltzmann HPC产品代码的分析作为示例，该代码针对几种架构进行了高度优化，现在也移植到了ARMv8。我们在一个基于生产硅片Cavium CN8890 SoC的系统上进行了测试。特别是，作为性能分析工具，我们采用了Extrae和Paraver，利用了我们最初为ThunderX平台开发的PAPI支持，现在也可以在上游使用。本文的贡献有两方面：首先，我们证明了独立于CPU提供商的标准HPC平台上可用的性能分析工具目前也可用于ARM SoC；其次，我们针对该平台实际优化了HPC应用程序，显示了与其他体系结构的相似之处。

在本文中，我们总结了在巴塞罗那超级计算中心为访问Cavium ThunderX CN8890 SoC的硬件性能计数器所做的工作，目的是在该平台上的标准HPC性能分析工具中利用这些计数器。所有生成的补丁现在都可以在各自项目的上游版本中使用。多亏了简单的微基准，我们已经能够识别出不符合其他ARM平台的计数器，如Tab中所述。I.尤其是PAPI_VEC_INS计数器目前在此架构上不应被认为是可信的。通过在Cavium ThunderX节点上的工作，我们展示了在ARM SoC上也可以利用其他架构(如Extrae和Paraver)上已经开发和使用的工具来执行高级性能分析，这些工具利用PAPI库来读取硬件性能计数器。特别是，我们已经展示了如何利用这些工具，分析了一个实际的HPC应用程序，该应用程序实现了一个在Cavium ThunderX上运行的、由INFN和Ferrara大学开发的Lattice Boltzmann模型。多亏了这一分析，我们能够识别导致此应用程序在Cavium ThunderX体系结构上性能提高的低效率(即，传播带宽增加了62%)。进一步的研究可能会导致其他优化，以获得更好的结果。最终，我们可以声明，在此平台上，现在可以执行标准HPC计算机上可用的大多数性能分析。我们还表明，对于本文中考虑的流体力学应用程序，针对其他多核架构(如Intel Knl)引入的优化也可以在ARM SoC(如Cavium ThunderX)上受益。我们计划将这项工作扩展到ThunderX芯片的第二个版本，预计该芯片将在HPC市场得到广泛采用，此外，我们还希望能够访问嵌入在此SoC中的非标准电源相关寄存器，就像访问其他体系结构所做的那样。使用PAPI读取功率数据的可能性也将使精细的能量分析成为可能，而不需要外部功率表。
```

### [22] A performance analysis of the first generation of  HPC‐optimized Arm processors

```
McIntosh‐Smith S, Price J, Deakin T, et al. A performance analysis of the first generation of HPC‐optimized Arm processors[J]. Concurrency and Computation: Practice and Experience, 2019, 31(16): e5110.
```

```
在这篇文章中，我们展示了Isambard的性能结果，这是第一台基于ARM CPU的生产型超级计算机，这些CPU已经针对高性能计算进行了优化。Isambard是第一个Cray XC50“侦察”系统，将Cavium ThunderX2基于ARM的CPU与Cray‘s Aries互连结合在一起。整个Isambard系统将于2018年夏天交付，届时它将包含1万多个ARM核心。在这项工作中，我们展示了2018年3月升级到B0测试版硅片的8个早期访问节点的节点级性能结果。我们提供了ThunderX2与主流CPU(包括Intel Skylake和Broadwell以及Xeon Phi)的节点级基准测试结果。我们专注于一系列对英国国家HPC服务Archer以及Isambard项目合作伙伴和更广泛的HPC社区至关重要的应用程序和迷你应用程序。我们还比较了可用于ARM的三个主要软件工具链的性能：Cray的CCE、ARM版本的Clang/Flang/LLVM和GNU。

本文提供的结果表明，基于ARM的处理器现在能够提供与现有供应商提供的最先进产品相媲美的性能水平，同时显著提高每美元的性能。我们的大多数基准测试开箱即可成功编译和运行，无需特定于体系结构的代码调优即可实现高性能。这代表着HPC ARM生态系统成熟的一个重要里程碑，这些处理器现在可以被视为未来采购的可行竞争者。未来的工作将使用完整的Isambard系统来评估在ThunderX2处理器上大规模运行的生产应用程序。我们在这篇论文中没有提到能源效率。我们早期的观察表明，ThunderX2的能效与我们测试的x86 CPU大致相同。这并不令人惊讶--对于给定的制造技术，一次翻转将占用一定数量的焦耳，而在芯片上移动一个字节一定距离也将消耗一定数量的能量。没有魔力，当大部分能量都花在移动数据和执行数值运算时，指令集体系结构对能效的影响很小。总体而言，这些结果表明，针对HPC进行了优化的基于ARM的服务器CPU现在是真正的生产系统选项，提供了与同类最佳CPU相媲美的性能，同时可能提供诱人的性价比优势。
```

### [23] SoC-based computing infrastructures for scientific applications and commercial services: Performance and economic evaluations

```
D’Agostino D, Quarati A, Clematis A, et al. SoC-based computing infrastructures for scientific applications and commercial services: Performance and economic evaluations[J]. Future Generation Computer Systems, 2019, 96: 11-22.
```

```
从传统的高性能计算中心到云数据中心，能源消耗是目前运营计算基础设施中最相关的问题之一。低功耗片上系统(SoC)体系结构最初是在移动和嵌入式技术的背景下发展起来的，由于其日益增长的计算性能以及相对较低的成本和功耗需求，对科学和工业应用也越来越有吸引力。在本文中，我们考察了最具代表性的SoC在计算密集型N-Body基准测试、简单的基于深度学习的应用和来自分子生物学领域的实际应用中的性能。目标是评估与现有基础设施中采用的传统服务器级架构相比，它们能够实现的科学和商业目的的解决方案的时间、能量到解决方案和经济方面的权衡。

本文对在计算基础设施中采用低功耗片上系统的性能和经济方面进行了分析。从两个基准测试，即广泛使用的N-Body算法和基于深度学习的应用程序出发，我们讨论了最先进的低功耗SoC体系结构与配备现有计算基础设施的传统服务器级CPU和GPU相比所能达到的性能。然后，我们将重点转移到评估有趣的原始、能量和计算数字是否也适用于现实生活中的应用场景。事实上，SoC是运行许多科学和商业应用程序的一种有趣的替代方案。在本文中，我们分析了一个场景，其中SME愿意使用基于SoC的基础设施来提供基于来自分子生物学领域的真实生活应用的服务。需要注意的是，将高端商业/HPC服务器与基于来自移动和嵌入式世界的低功耗SoC的主板进行比较可能被认为是不公平的，但所提供的结果也评估了对于耗时的应用，如NGS数据分析，从解决方案的时间到解决方案的能量到解决方案的经济方面的折衷而言，使用低功耗架构是一种可行的选择。这项工作的未来发展是双重的。我们将从一个方面评估来自具有不同需求的应用领域的其他用例。特别是，我们正在考虑将这种低功耗架构与边缘计算范式相结合，用于物联网应用和制药行业，通过对蛋白质-化学物质相互作用的计算机模拟来开发新药。此外，我们计划通过在总拥有成本中包括更广泛的元素，例如网络互连、冷却和开发能够利用如此多的低功耗计算节点的应用程序的成本，来扩展在计算基础设施中采用基于SoC的集群的分析。这方面与并行计算社区的研究工作部分重叠，为亿级计算铺平了道路。
```

### [24] 借势 ARM 挑战 Intel？一窥中国芯发展现状

```
铁流. 借势 ARM 挑战 Intel? 一窥中国芯发展现状[J]. 微型计算机, 2017 (15): 104-106.
```

```
国内将 ARM 处理器引入大规模 HPC 集群的时间较晚，发展较好的 ARM 芯片厂商包括申微、天津飞腾、华芯通和华为等。ARM 面向 HPC 的相关研究在近五年内才逐渐增多，这些工作对本文进行 ARM 系统的性能评测、应用移植和优化研究具有很大启发。
```

### [25] 基于ARM V8 平台的多维FFT 实现与优化研究

```
陈暾, 李志豪, 贾海鹏, 等. 基于 ARM V8 平台的多维 FFT 实现与优化研究[J]. 计算机学报, 2019, 11.
```

```
ARM V8 是首款支持 64 位指令集的 ARM 处理器架构，其计算能力获得了极大提升，应用领域也更加广泛。FFT（快速傅里叶变换）是用于计算离散傅里叶变换（DFT）或其逆运算的快速算法，它广泛应用于工程，科学和数学计算。到目前为止，鲜有基于 ARM 平台的高性能 FFT 算法的实现和优化，然而，随着 ARM V8 处理器应用的日益广泛，研究 FFT算法在 ARM 平台上高性能实现日益重要。本文在 ARM V8 平台上实现和优化了一个高性能的多维 FFT 算法库：PerfFFT，通过 FFT 蝶形网络优化、蝶形计算优化、蝶形自动生成、SIMD 优化、内存对齐、Cache-aware 的分块算法和高效转置等优化方法的应用，显著提升了 FFT 算法的性能。实验结果表明，PerfFFT 相比目前应用最为广泛的开源 FFT 库 FFTW3.3.6 实现了 10%~591%的性能提升，而相比 ARM 高性能商业库 ARM Performance Library 实现了 13%~44%的性能提升。

本文在实现多维 FFT 变换的基础上，提出了蝶形网络优化、蝶形计算优化、蝶形自动生成、SIMD优化、内存对齐、Cache-Aware 的分块算法和高效转置算法等优化方法，实现了一个 ARM V8 平台上的高性能的多维 FFT 库：PerfFFT。与现有的高性能 FFT 软件库 FFTW、ARMPL 相比，PerfFFT 性能都明显高于这两个库的性能。这不仅实现了提升ARM V8 平台 FFT 软件库性能的目标，而且为 ARM V8 平台上程序优化提供了新的思路。未来的主要工作将从双精度 FFT 算法的实现和优化、三维 FFT减少 TLB 缺失率优化，两个首要的入手点，最后，为 FFT 软件库搭建自适应框架也未来一个重要的工作，我们着眼于手工优化和自适应优化技术相结合的方式，进一步提升 FFT 算法库的性能。
```

### [26] 一种 ARM 处理器面向高性能计算的性能评估

```
王一超, 廖秋承, 左思成, 等. 一种 ARM 处理器面向高性能计算的性能评估[J]. 计算机科学, 2019, 46(8): 95-99.
```

```
上海交通大学网络信息中心的王一超等人在 2019 年对一款面向 HPC 的ARM 处理器做了较为全面的性能评估。该文章从微架构层面选取了 HPL 和STREAM 等基准测试指标，从应用层面选取了 CloverLeaf、TeaLeaf、SNAP 和Neutral 四款科学应用进行 ARM 处理器性能评估，同时将性能与主流商用的 Xeno处理器进行了比较。结果表明 ARM 处理器在“绿色计算”领域具有竞争力，但在计算密集型和随机访存型的应用支持上仍需要更多的改进。
```

### [27] 飞腾处理器与商用处理器性能比较

```
方建滨, 杜琦, 唐滔, 等. 飞腾处理器与商用处理器性能比较[J]. 计算机工程与科学, 2019, 41(1): 1-8.
```

```
国防科技大学的方建滨等人类似地从微基准测试和应用层面两方面深入分析了国产基于 ARM 架构的飞腾 FT-1500A 型和商用 Intel 至强处理器的性能差异，并研究了将一个开源代码移植到两个系统的方法，讨论了在这两个系统的单核和多核性能，并依据实验结果提出了优化建议。作者认为 FT-1500A 飞腾处理器已具备良好的生态基础（易用的操作系统、多种编译器和易用的工具链），这使得移植典型科学计算应用变得简单可行，此外还指出了此款 ARM 处理器在性能上的不足之处。
```

### [28] An empirical study of hpc workloads on huawei kunpeng 916 processor

```
Wang Y C, Chen J K, Li B R, et al. An empirical study of hpc workloads on huawei kunpeng 916 processor[C]//2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS). IEEE, 2019: 360-367.
```

```
基于ARM的服务器处理器在高性能计算(HPC)方面获得了发展势头。虽然不是专门为HPC设计的，但华为鲲鹏916处理器拥有32个ARMv8内核，对HPC工作负载很有吸引力。然而，它的潜力仍然未知。为了透彻地了解其潜力，我们分三个步骤进行了系统的评估：1)三个著名的基准测试(HPL、STREAM和LMbench)；2)三个典型的科学内核(SpMV、N-Body和GEMM)；3)三个广泛使用的迷你应用程序(TeaLeaf、Neual和SNAP)和一个真实世界的应用程序GTC-P。我们将鲲鹏916与Intel Xeon E5-2680v3/4(Haswell/Broadwell)的性能结果进行了比较。评估结果表明，鲲鹏916的内存带宽高于两个Intel处理器，因此可以在运行内存受限的高性能计算应用程序时获得令人信服的性能。
```

